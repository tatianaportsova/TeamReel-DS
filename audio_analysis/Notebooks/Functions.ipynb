{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora\n",
    "import shutil\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import re \n",
    "import speech_recognition as sr\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import moviepy.editor\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import keras\n",
    "import wave\n",
    "import contextlib\n",
    "from keras.models import Model, model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import glob \n",
    "import sys\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the ML model, weights, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load models\n",
    "# loading json and model architecture \n",
    "json_file = open('ML_Model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"Augmented_Model.h5\")\n",
    " \n",
    "# compile loaded model\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# load lables\n",
    "infile = open('labels','rb')\n",
    "lb = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_from_video():\n",
    "    video = moviepy.editor.VideoFileClip('lecture_clip.mp4')\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile('audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_analysis():\n",
    "    get_audio_from_video()\n",
    "    audio = AudioSegment.from_file(\"audio.wav\", \"wav\")\n",
    "    chunk_length_ms = 4000\n",
    "    chunks = make_chunks(audio, chunk_length_ms)\n",
    "    \n",
    "    #Export all of the individual chunks as wav files\n",
    "    path = \"audio_sentiment/\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "        \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_name = \"audio_sentiment/chunk{0}.wav\".format(i)\n",
    "        chunk.export(chunk_name, format=\"wav\")\n",
    "    \n",
    "    lists_of_files = os.listdir(path)\n",
    "    test_predictions = pd.DataFrame(columns=['predictions'])\n",
    "    \n",
    "    for filename in lists_of_files:\n",
    "        try:\n",
    "            data, sample_rate = librosa.load(\"audio_sentiment/{}\".format(filename),\n",
    "                                             res_type='kaiser_fast',\n",
    "                                             sr=44100)\n",
    "            #print(filename)\n",
    "            sample_rate = np.array(sample_rate)\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "            newdf = pd.DataFrame(data=mfccs).T\n",
    "            \n",
    "            # Apply predictions\n",
    "            newdf= np.expand_dims(newdf, axis=2)\n",
    "            newpred = loaded_model.predict(newdf,batch_size=16,verbose=1)\n",
    "            \n",
    "            # Get the final predicted label\n",
    "            final = newpred.argmax(axis=1)\n",
    "            final = final.astype(int).flatten()\n",
    "            final = (lb.inverse_transform((final)))\n",
    "            #print(final)\n",
    "            test_predictions.loc[filename] = final\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    labels = test_predictions.predictions.value_counts(normalize=True).keys().tolist()\n",
    "    \n",
    "    values= []\n",
    "    predictions = {} \n",
    "    for i in test_predictions.predictions.value_counts(normalize=True).tolist():\n",
    "        values.append(round(i,2))   \n",
    "    for key in labels:\n",
    "        for value in values:\n",
    "            predictions[key] = value\n",
    "            values.remove(value) \n",
    "            break\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Speed of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_audio_file(file_name = 'audio.wav'):\n",
    "    \"\"\"\n",
    "    Breaks an audio file into smaller chunks\n",
    "    \"\"\"\n",
    "    myaudio = AudioSegment.from_file(file_name, \"wav\") \n",
    "    chunk_length_ms = 20000 # pydub calculates in millisec\n",
    "    chunks = make_chunks(myaudio, chunk_length_ms) #Make chunks of 20 sec\n",
    "\n",
    "    #Export all of the individual chunks as wav files\n",
    "    path = \"audio_chunks/\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_name = \"audio_chunks/chunk{0}.wav\".format(i)\n",
    "        chunk.export(chunk_name, format=\"wav\")\n",
    "        \n",
    "dirname = r\"audio_chunks/\"\n",
    "def get_file_paths(dirname):\n",
    "    \"\"\"\n",
    "    Gets file paths from the directory\n",
    "    \"\"\"\n",
    "    file_paths = []  \n",
    "    for root, directories, files in os.walk(dirname):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)  \n",
    "    return file_paths \n",
    "\n",
    "def process_file(file):\n",
    "    \"\"\"\n",
    "    Applies SpeechRecognition to the audio files\n",
    "    \"\"\"\n",
    "    r = sr.Recognizer()\n",
    "    a = ''\n",
    "    with sr.AudioFile(file) as source:\n",
    "        #r.adjust_for_ambient_noise(source)  # adjust for noisy audio\n",
    "        audio = r.record(source)    \n",
    "        try:\n",
    "            a =  r.recognize_google(audio)   # recognize_google_cloud\n",
    "        except sr.UnknownValueError:\n",
    "            a = \"Google Speech Recognition could not understand audio\"\n",
    "        except sr.RequestError as e:\n",
    "            a = \"Could not request results from Google Speech Recognition service; {0}\".format(e)  \n",
    "    return a\n",
    "\n",
    "\n",
    "def get_text():\n",
    "    # Create a new directory to store the chunks of txt\n",
    "    path = 'text_chunks/'\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "    \n",
    "    files = get_file_paths(dirname)                             \n",
    "    for file in files:                                           \n",
    "        (filepath, ext) = os.path.splitext(file)                  \n",
    "        file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        # only interested if extension is '.wav'\n",
    "        if ext == '.wav':                                         \n",
    "            a = process_file(file)\n",
    "            with open(\"text_chunks/{}.txt\".format(file_name), \"w\") as f:\n",
    "                f.write(a+\". \")                            \n",
    "\n",
    "\n",
    "def get_transcripts_from_audio(audio_file='audio.wav'):\n",
    "    \"\"\"\n",
    "    A function to get a transcripts from the audio (stores text into separate chunks of text)\n",
    "    \"\"\"\n",
    "    break_audio_file()\n",
    "    get_text()\n",
    "    \n",
    "path = r\"text_chunks/\"\n",
    "def get_combined_text(path):\n",
    "    \"\"\"\n",
    "    Combines all of the separate chunks of text into one file\n",
    "    \"\"\"\n",
    "    files = get_file_paths(path)\n",
    "    files.sort(key=lambda x: int(re.sub('\\D', '', x)))\n",
    "    #sorted(files, key=lambda x: int(re.sub('\\D', '', x)))\n",
    "    with open('outputfile.txt', 'w+') as f: \n",
    "        for file in files:\n",
    "            with open(file) as infile:\n",
    "                f.write(infile.read()+'\\n')\n",
    "                \n",
    "# Tokenize data\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text)]   # (if token not in STOPWORDS)\n",
    "\n",
    "def gather_data(path_to_data): \n",
    "    data = []\n",
    "    for f in os.listdir(path):\n",
    "        if os.path.isdir(f) == False:\n",
    "            if f[-3:] == 'txt':\n",
    "                with open(os.path.join(path,f)) as t:\n",
    "                    text = t.read().strip('\\n')\n",
    "                    data.append(tokenize(str(text)))       \n",
    "    return data\n",
    "\n",
    "def get_audio_duration(file_name = 'audio.wav'):\n",
    "    with contextlib.closing(wave.open(file_name,'r')) as f:\n",
    "        frames = f.getnframes()\n",
    "        rate = f.getframerate()\n",
    "        duration = (frames / float(rate)) / 60\n",
    "        return duration\n",
    "    \n",
    "def get_speed_of_speech():\n",
    "    tokens = gather_data('outputfile.txt')\n",
    "    \n",
    "    count = 0\n",
    "    for i in tokens:\n",
    "        count += len(i)\n",
    "        \n",
    "    speed_of_speech = count / get_audio_duration(file_name = 'audio.wav')\n",
    "    return speed_of_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to remove directories and files after we are done with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files():\n",
    "    \"\"\"\n",
    "    Removes files and directories after they're no longer needed\n",
    "    \"\"\"\n",
    "    paths = ['audio_chunks/', 'audio_sentiment/', 'text_chunks/']\n",
    "    os.remove('outputfile.txt')\n",
    "    \n",
    "    for i in paths:\n",
    "        try:\n",
    "            shutil.rmtree(i)\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s : %s\" % (i, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main function to analyse the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_audio():\n",
    "    path = r\"text_chunks/\"\n",
    "    predictions = get_sentiment_analysis()\n",
    "    get_transcripts_from_audio(audio_file='audio.wav')\n",
    "    get_combined_text(path)\n",
    "    speed_of_speech = get_speed_of_speech()\n",
    "    return predictions, speed_of_speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   6%|â–Œ         | 253/4562 [00:00<00:01, 2525.91it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Successfully created the directory audio_sentiment/ \n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "Successfully created the directory audio_chunks/ \n",
      "Successfully created the directory text_chunks/ \n"
     ]
    }
   ],
   "source": [
    "data = analyse_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'positive': 0.41, 'neutral': 0.31, 'negative': 0.27}, 140.3780151786146)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3-6",
   "language": "python",
   "name": "python-3-6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
